{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdadd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "assert load_dotenv(find_dotenv(usecwd=False)), \"The .env file was not loaded.\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import drn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from generate_synthetic_dataset import generate_synthetic_gamma_lognormal, generate_synthetic_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(\n",
    "    features: pd.DataFrame,\n",
    "    target: pd.Series,\n",
    "    seed: int = 42,\n",
    "    train_size: float = 0.6,\n",
    "    val_size: float = 0.2,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"\n",
    "    Split features and target into train, validation, and test sets based on fractions of the entire dataset.\n",
    "\n",
    "    Args:\n",
    "        features: DataFrame of predictors.\n",
    "        target: Series of labels.\n",
    "        seed: Random seed for reproducibility.\n",
    "        train_size: Fraction of data for training.\n",
    "        val_size: Fraction of data for validation.\n",
    "            (test_size is computed as 1 - train_size - val_size)\n",
    "    Returns:\n",
    "        x_train_raw, x_val_raw, x_test_raw,\n",
    "        y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    # Compute test fraction\n",
    "    test_size = 1.0 - train_size - val_size\n",
    "    if test_size <= 0:\n",
    "        raise ValueError(\n",
    "            f\"train_size + val_size must be < 1. Got {train_size + val_size}\"\n",
    "        )\n",
    "\n",
    "    # Split off test set\n",
    "    x_train_val, x_test_raw, y_train_val, y_test = train_test_split(\n",
    "        features, target, test_size=test_size, random_state=seed, shuffle=True\n",
    "    )\n",
    "    # Split train+val into train and val\n",
    "    relative_val_size = val_size / (train_size + val_size)\n",
    "    x_train_raw, x_val_raw, y_train, y_val = train_test_split(\n",
    "        x_train_val,\n",
    "        y_train_val,\n",
    "        test_size=relative_val_size,\n",
    "        random_state=seed,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    return x_train_raw, x_val_raw, x_test_raw, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def generate_categories(\n",
    "    x_train_raw: pd.DataFrame,\n",
    "    x_val_raw: pd.DataFrame,\n",
    "    x_test: pd.DataFrame,\n",
    "    cat_features: list[str],\n",
    ") -> dict[str, list]:\n",
    "    \"\"\"\n",
    "    Create a mapping of categorical features to their full category lists:\n",
    "      - Initialize from training split\n",
    "      - Detect new categories in val/test, print a warning, and extend\n",
    "    Returns:\n",
    "        all_categories: feature -> sorted list of categories\n",
    "    \"\"\"\n",
    "    all_categories = {\n",
    "        feature: sorted(x_train_raw[feature].dropna().unique())\n",
    "        for feature in cat_features\n",
    "    }\n",
    "    for split_name, df in [(\"validation\", x_val_raw), (\"test\", x_test)]:\n",
    "        for feature in cat_features:\n",
    "            seen = set(all_categories[feature])\n",
    "            unique_vals = set(df[feature].dropna().unique())\n",
    "            new_vals = unique_vals - seen\n",
    "            if new_vals:\n",
    "                print(\n",
    "                    f\"New categories for '{feature}' in {split_name} split: {new_vals}\"\n",
    "                )\n",
    "                all_categories[feature] = sorted(\n",
    "                    all_categories[feature] + list(new_vals)\n",
    "                )\n",
    "    return all_categories\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "    x_train_raw: pd.DataFrame,\n",
    "    x_val_raw: pd.DataFrame,\n",
    "    x_test_raw: pd.DataFrame,\n",
    "    num_features: list[str],\n",
    "    cat_features: list[str],\n",
    "    num_standard: bool = True,\n",
    ") -> tuple[\n",
    "    pd.DataFrame, pd.DataFrame, pd.DataFrame, ColumnTransformer, dict[str, list]\n",
    "]:\n",
    "    \"\"\"\n",
    "    Fit a ColumnTransformer on x_train_raw and transform raw splits.\n",
    "    - Numeric features are optionally standardized.\n",
    "    - Categorical features are one-hot encoded, using full categories detected from splits.\n",
    "\n",
    "    Returns:\n",
    "        x_train, x_val, x_test, fitted ColumnTransformer, all_categories mapping\n",
    "    \"\"\"\n",
    "    # Prepare category mapping\n",
    "    all_categories = generate_categories(\n",
    "        x_train_raw, x_val_raw, x_test_raw, cat_features\n",
    "    )\n",
    "\n",
    "    # OneHotEncoder with fixed categories\n",
    "    ohe = OneHotEncoder(\n",
    "        categories=[all_categories[f] for f in cat_features],\n",
    "        handle_unknown=\"error\",\n",
    "        sparse_output=False,\n",
    "    )\n",
    "\n",
    "    # Build transformers list\n",
    "    transformers = [\n",
    "        (\"num\", StandardScaler() if num_standard else \"passthrough\", num_features),\n",
    "        (\"cat\", ohe, cat_features),\n",
    "    ]\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=transformers, remainder=\"drop\", verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    # Fit & transform splits\n",
    "    x_train_arr = ct.fit_transform(x_train_raw)\n",
    "    x_val_arr = ct.transform(x_val_raw)\n",
    "    x_test_arr = ct.transform(x_test_raw)\n",
    "\n",
    "    # Build DataFrames with proper feature names\n",
    "    feature_names = ct.get_feature_names_out()\n",
    "    x_train = pd.DataFrame(x_train_arr, columns=feature_names)\n",
    "    x_val = pd.DataFrame(x_val_arr, columns=feature_names)\n",
    "    x_test = pd.DataFrame(x_test_arr, columns=feature_names)\n",
    "\n",
    "    return x_train, x_val, x_test, ct, all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17631f",
   "metadata": {},
   "source": [
    "# Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd29ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target, means, dispersion = generate_synthetic_gamma_lognormal(20000, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96aab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw, x_val_raw, x_test_raw, y_train_raw, y_val_raw, y_test_raw = split_data(\n",
    "    features,\n",
    "    target,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5069fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is to generate a larger test dataset (since it is just cheap synthetic data).\n",
    "# May want to harmonise this with the original generation process, though it will change the train/val/test sets.\n",
    "features_new, target_new, _, _ = generate_synthetic_gamma_lognormal(50000, seed=0)\n",
    "processed = drn.split_and_preprocess(\n",
    "    features_new, target_new, [\"X_1\", \"X_2\"], [], seed=102, num_standard=False\n",
    ")\n",
    "x_test_raw, y_test_raw = processed[2], processed[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATA_DIR = Path(\"data/interim/synth\")\n",
    "SPLIT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train_raw.to_csv(SPLIT_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val_raw.to_csv(SPLIT_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test_raw.to_csv(SPLIT_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train_raw.to_csv(SPLIT_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val_raw.to_csv(SPLIT_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test_raw.to_csv(SPLIT_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b275fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them back up from the files\n",
    "x_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_train.csv\")\n",
    "x_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_val.csv\")\n",
    "x_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_test.csv\")\n",
    "y_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_train.csv\")\n",
    "y_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_val.csv\")\n",
    "y_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b82002",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, ct, all_categories = preprocess_data(\n",
    "    x_train_raw,\n",
    "    x_val_raw,\n",
    "    x_test_raw,\n",
    "    num_features=[\"X_1\", \"X_2\"],\n",
    "    cat_features=[],\n",
    "    num_standard=False,\n",
    ")\n",
    "\n",
    "# Not doing anything extra to the target variable\n",
    "y_train = y_train_raw\n",
    "y_val = y_val_raw\n",
    "y_test = y_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0661e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_DIR = Path(\"data/processed/synth\")\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train.to_csv(PROCESSED_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val.to_csv(PROCESSED_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test.to_csv(PROCESSED_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train.to_csv(PROCESSED_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val.to_csv(PROCESSED_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test.to_csv(PROCESSED_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22367d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw, x_val_raw, x_test_raw, y_train_raw, y_val_raw, y_test_raw = split_data(\n",
    "    features,\n",
    "    target,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb19be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is to generate a larger test dataset (since it is just cheap synthetic data).\n",
    "# May want to harmonise this with the original generation process, though it will change the train/val/test sets.\n",
    "features_new, target_new, _, _ = generate_synthetic_gamma_lognormal(50000, seed=0)\n",
    "processed = drn.split_and_preprocess(\n",
    "    features_new, target_new, [\"X_1\", \"X_2\"], [], seed=102, num_standard=False\n",
    ")\n",
    "x_test_raw, y_test_raw = processed[2], processed[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4666924",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATA_DIR = Path(\"data/interim/synth\")\n",
    "SPLIT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train_raw.to_csv(SPLIT_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val_raw.to_csv(SPLIT_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test_raw.to_csv(SPLIT_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train_raw.to_csv(SPLIT_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val_raw.to_csv(SPLIT_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test_raw.to_csv(SPLIT_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de282fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them back up from the files\n",
    "x_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_train.csv\")\n",
    "x_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_val.csv\")\n",
    "x_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_test.csv\")\n",
    "y_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_train.csv\")\n",
    "y_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_val.csv\")\n",
    "y_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, ct, all_categories = preprocess_data(\n",
    "    x_train_raw,\n",
    "    x_val_raw,\n",
    "    x_test_raw,\n",
    "    num_features=[\"X_1\", \"X_2\"],\n",
    "    cat_features=[],\n",
    "    num_standard=False,\n",
    ")\n",
    "\n",
    "# Not doing anything extra to the target variable\n",
    "y_train = y_train_raw\n",
    "y_val = y_val_raw\n",
    "y_test = y_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0247da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_DIR = Path(\"data/processed/synth\")\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train.to_csv(PROCESSED_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val.to_csv(PROCESSED_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test.to_csv(PROCESSED_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train.to_csv(PROCESSED_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val.to_csv(PROCESSED_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test.to_csv(PROCESSED_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31369574",
   "metadata": {},
   "source": [
    "# Real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/raw/freMPL1.csv\")\n",
    "\n",
    "claims = df.loc[df[\"ClaimAmount\"] > 0, :]\n",
    "\n",
    "# Scaling\n",
    "target = claims[\"ClaimAmount\"] / 1000\n",
    "features = claims.drop(\"ClaimAmount\", axis=1)\n",
    "features = features.drop(\n",
    "    [\"RecordBeg\", \"RecordEnd\", \"ClaimInd\", \"Garage\"], axis=1\n",
    ")  # Drop garage due to missing values\n",
    "\n",
    "# Convert \"VehAge\" categories to numeric\n",
    "features[\"VehAge\"] = features[\"VehAge\"].map(\n",
    "    {\n",
    "        \"0\": 0,\n",
    "        \"1\": 1,\n",
    "        \"2\": 2,\n",
    "        \"3\": 3,\n",
    "        \"4\": 4,\n",
    "        \"5\": 5,\n",
    "        \"6-7\": 6,\n",
    "        \"8-9\": 8,\n",
    "        \"10+\": 11,\n",
    "    }\n",
    ")\n",
    "\n",
    "speed_ranges = [speed for speed in np.unique(features[\"VehMaxSpeed\"])]\n",
    "speed_series = pd.Series(speed_ranges)\n",
    "mapping = {speed_range: i + 1 for i, speed_range in enumerate(speed_ranges)}\n",
    "features[\"VehMaxSpeed\"] = features[\"VehMaxSpeed\"].map(mapping)\n",
    "features[\"SocioCateg\"] = features[\"SocioCateg\"].str.extract(\"(\\d+)\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw, x_val_raw, x_test_raw, y_train_raw, y_val_raw, y_test_raw = split_data(\n",
    "    features,\n",
    "    target,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATA_DIR = Path(\"data/interim/real\")\n",
    "SPLIT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train_raw.to_csv(SPLIT_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val_raw.to_csv(SPLIT_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test_raw.to_csv(SPLIT_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train_raw.to_csv(SPLIT_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val_raw.to_csv(SPLIT_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test_raw.to_csv(SPLIT_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34f3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them back up from the files\n",
    "x_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_train.csv\")\n",
    "x_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_val.csv\")\n",
    "x_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_test.csv\")\n",
    "y_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_train.csv\")\n",
    "y_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_val.csv\")\n",
    "y_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\n",
    "    \"HasKmLimit\",\n",
    "    \"Gender\",\n",
    "    \"MariStat\",\n",
    "    \"VehUsage\",\n",
    "    \"VehBody\",\n",
    "    \"VehPrice\",\n",
    "    \"VehEngine\",\n",
    "    \"VehEnergy\",\n",
    "    \"VehClass\",\n",
    "    \"SocioCateg\",\n",
    "]\n",
    "\n",
    "num_features = [\n",
    "    feature for feature in x_train_raw.columns if feature not in cat_features\n",
    "]\n",
    "\n",
    "x_train, x_val, x_test, ct, all_categories = preprocess_data(\n",
    "    x_train_raw,\n",
    "    x_val_raw,\n",
    "    x_test_raw,\n",
    "    num_features=num_features,\n",
    "    cat_features=cat_features,\n",
    "    num_standard=True,\n",
    ")\n",
    "\n",
    "# Not doing anything extra to the target variable\n",
    "y_train = y_train_raw\n",
    "y_val = y_val_raw\n",
    "y_test = y_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff74a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_DIR = Path(\"data/processed/real\")\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train.to_csv(PROCESSED_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val.to_csv(PROCESSED_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test.to_csv(PROCESSED_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train.to_csv(PROCESSED_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val.to_csv(PROCESSED_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test.to_csv(PROCESSED_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b114a",
   "metadata": {},
   "source": [
    "# Regularisation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target, means, dispersion = generate_synthetic_gaussian(40000)\n",
    "df = pd.concat([features, target], axis=1)\n",
    "df.to_csv(\"data/raw/reg.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7c229",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw, x_val_raw, x_test_raw, y_train_raw, y_val_raw, y_test_raw = split_data(\n",
    "    features,\n",
    "    target,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATA_DIR = Path(\"data/interim/reg\")\n",
    "SPLIT_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train_raw.to_csv(SPLIT_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val_raw.to_csv(SPLIT_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test_raw.to_csv(SPLIT_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train_raw.to_csv(SPLIT_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val_raw.to_csv(SPLIT_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test_raw.to_csv(SPLIT_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7037efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load them back up from the files\n",
    "x_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_train.csv\")\n",
    "x_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_val.csv\")\n",
    "x_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"x_test.csv\")\n",
    "y_train_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_train.csv\")\n",
    "y_val_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_val.csv\")\n",
    "y_test_raw = pd.read_csv(SPLIT_DATA_DIR / \"y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd487f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_test, ct, all_categories = preprocess_data(\n",
    "    x_train_raw,\n",
    "    x_val_raw,\n",
    "    x_test_raw,\n",
    "    num_features=[\"X_1\", \"X_2\"],\n",
    "    cat_features=[],\n",
    "    num_standard=False,\n",
    ")\n",
    "\n",
    "# Not doing anything extra to the target variable\n",
    "y_train = y_train_raw\n",
    "y_val = y_val_raw\n",
    "y_test = y_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dccf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_DIR = Path(\"data/processed/reg\")\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "x_train.to_csv(PROCESSED_DATA_DIR / \"x_train.csv\", index=False)\n",
    "x_val.to_csv(PROCESSED_DATA_DIR / \"x_val.csv\", index=False)\n",
    "x_test.to_csv(PROCESSED_DATA_DIR / \"x_test.csv\", index=False)\n",
    "y_train.to_csv(PROCESSED_DATA_DIR / \"y_train.csv\", index=False)\n",
    "y_val.to_csv(PROCESSED_DATA_DIR / \"y_val.csv\", index=False)\n",
    "y_test.to_csv(PROCESSED_DATA_DIR / \"y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
