{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from drn import DRNExplainer, crps, rmse, split_and_preprocess\n",
    "\n",
    "from analysis_utils import (\n",
    "    calibration_plot,\n",
    "    crps_wilcoxon_test,\n",
    "    nll_wilcoxon_test,\n",
    "    ql90_wilcoxon_test,\n",
    "    quantile_losses_raw,\n",
    "    quantile_points,\n",
    "    quantile_residuals_plots,\n",
    "    rmse_wilcoxon_test,\n",
    ")\n",
    "\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "plt.rcParams[\"xtick.labelsize\"] = 15\n",
    "plt.rcParams[\"ytick.labelsize\"] = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"models/real\")\n",
    "PLOT_DIR = Path(\"plots/real\")\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec 6.1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = \"freMPL1.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "claims = df.loc[df[\"ClaimAmount\"] > 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims[\"ClaimAmount\"].plot.density(color=\"green\", xlim=(0, 30000))\n",
    "# Setting the title with a larger font size\n",
    "plt.title(\"Empirical Density of Truncated Claims\", fontsize=20)\n",
    "\n",
    "# Setting the labels for x and y axes with larger font sizes\n",
    "plt.xlabel(\"Claim Amount ($)\", fontsize=15)\n",
    "plt.ylabel(\"\", fontsize=15)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.savefig(PLOT_DIR / \"Empirical Density (Real).png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "target = claims[\"ClaimAmount\"] / 1000\n",
    "features = claims.drop(\"ClaimAmount\", axis=1)\n",
    "features = features.drop(\n",
    "    [\"RecordBeg\", \"RecordEnd\", \"ClaimInd\", \"Garage\"], axis=1\n",
    ")  # Drop garage due to missing values\n",
    "\n",
    "# Convert \"VehAge\" categories to numeric\n",
    "features[\"VehAge\"] = features[\"VehAge\"].map(\n",
    "    {\n",
    "        \"0\": 0,\n",
    "        \"1\": 1,\n",
    "        \"2\": 2,\n",
    "        \"3\": 3,\n",
    "        \"4\": 4,\n",
    "        \"5\": 5,\n",
    "        \"6-7\": 6,\n",
    "        \"8-9\": 8,\n",
    "        \"10+\": 11,\n",
    "    }\n",
    ")\n",
    "feature_names = features.columns\n",
    "\n",
    "speed_ranges = [speed for speed in np.unique(features[\"VehMaxSpeed\"])]\n",
    "speed_series = pd.Series(speed_ranges)\n",
    "mapping = {speed_range: i + 1 for i, speed_range in enumerate(speed_ranges)}\n",
    "features[\"VehMaxSpeed\"] = features[\"VehMaxSpeed\"].map(mapping)\n",
    "features[\"SocioCateg\"] = features[\"SocioCateg\"].str.extract(\"(\\d+)\").astype(int)\n",
    "\n",
    "cat_features = [\n",
    "    \"HasKmLimit\",\n",
    "    \"Gender\",\n",
    "    \"MariStat\",\n",
    "    \"VehUsage\",\n",
    "    \"VehBody\",\n",
    "    \"VehPrice\",\n",
    "    \"VehEngine\",\n",
    "    \"VehEnergy\",\n",
    "    \"VehClass\",\n",
    "    \"SocioCateg\",\n",
    "]\n",
    "\n",
    "num_features = [feature for feature in features.columns if feature not in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and preprocess the data\n",
    "(\n",
    "    x_train,\n",
    "    x_val,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    x_train_raw,\n",
    "    x_val_raw,\n",
    "    x_test_raw,\n",
    "    num_features,\n",
    "    cat_features,\n",
    "    all_categories,\n",
    "    ct,\n",
    ") = split_and_preprocess(features, target, num_features, cat_features, seed=0)\n",
    "\n",
    "# Calculate and print statistics for y_train, y_val, y_test\n",
    "np.max(y_train), np.median(y_train), np.max(y_val), np.median(y_val), np.max(\n",
    "    y_test\n",
    "), np.median(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(x_train.values)\n",
    "Y_train = torch.Tensor(y_train.values)\n",
    "X_val = torch.Tensor(x_val.values)\n",
    "Y_val = torch.Tensor(y_val.values)\n",
    "X_test = torch.Tensor(x_test.values)\n",
    "Y_test = torch.Tensor(y_test.values)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = torch.load(MODEL_DIR / \"glm.pkl\", weights_only=False)\n",
    "cann = torch.load(MODEL_DIR / \"cann.pkl\", weights_only=False)\n",
    "mdn = torch.load(MODEL_DIR / \"mdn.pkl\", weights_only=False)\n",
    "ddr = torch.load(MODEL_DIR / \"ddr.pkl\", weights_only=False)\n",
    "drn = torch.load(MODEL_DIR / \"drn.pkl\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec 6.3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"GLM\", \"CANN\", \"MDN\", \"DDR\", \"DRN\"]\n",
    "models = [glm, cann, mdn, ddr, drn]\n",
    "\n",
    "print(\"Generating distributional forecasts\")\n",
    "dists_train = {}\n",
    "dists_val = {}\n",
    "dists_test = {}\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    print(f\"- {name}\")\n",
    "    dists_train[name] = model.distributions(X_train)\n",
    "    dists_val[name] = model.distributions(X_val)\n",
    "    dists_test[name] = model.distributions(X_test)\n",
    "\n",
    "print(\"Calculating CDF over a grid\")\n",
    "GRID_SIZE = 3000  # Increase this to get more accurate CRPS estimates\n",
    "grid = torch.linspace(0, np.max(y_train) * 1.1, GRID_SIZE).unsqueeze(-1)\n",
    "\n",
    "cdfs_train = {}\n",
    "cdfs_val = {}\n",
    "cdfs_test = {}\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    print(f\"- {name}\")\n",
    "    cdfs_train[name] = dists_train[name].cdf(grid)\n",
    "    cdfs_val[name] = dists_val[name].cdf(grid)\n",
    "    cdfs_test[name] = dists_test[name].cdf(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating negative loglikelihoods\")\n",
    "nlls_train = {}\n",
    "nlls_val = {}\n",
    "nlls_test = {}\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    nlls_train[name] = -dists_train[name].log_prob(Y_train).mean()\n",
    "    nlls_val[name] = -dists_val[name].log_prob(Y_val).mean()\n",
    "    nlls_test[name] = -dists_test[name].log_prob(Y_test).mean()\n",
    "\n",
    "\n",
    "for nll_dict, df_name in zip(\n",
    "    [nlls_train, nlls_val, nlls_test], [\"training\", \"val\", \"test\"]\n",
    "):\n",
    "    print(f\"NLL on {df_name} set\")\n",
    "    for name, model in zip(names, models):\n",
    "        print(f\"{name}: {nll_dict[name].mean():.4f}\")\n",
    "    print(f\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_wilcoxon_test(dists_val, Y_val, \"Validation\")\n",
    "nll_wilcoxon_test(dists_test, Y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating CRPS\")\n",
    "grid = grid.squeeze()\n",
    "crps_train = {}\n",
    "crps_val = {}\n",
    "crps_test = {}\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    crps_train[name] = crps(Y_train, grid, cdfs_train[name])\n",
    "    crps_val[name] = crps(Y_val, grid, cdfs_val[name])\n",
    "    crps_test[name] = crps(Y_test, grid, cdfs_test[name])\n",
    "\n",
    "for crps_dict, df_name in zip(\n",
    "    [crps_train, crps_val, crps_test], [\"training\", \"val\", \"test\"]\n",
    "):\n",
    "    print(f\"CRPS on {df_name} set\")\n",
    "    for name, model in zip(names, models):\n",
    "        print(f\"{name}: {crps_dict[name].mean():.4f}\")\n",
    "    print(f\"------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_wilcoxon_test(cdfs_val, Y_val, grid, \"Validation\")\n",
    "crps_wilcoxon_test(cdfs_test, Y_test, grid, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_train = {}\n",
    "rmse_val = {}\n",
    "rmse_test = {}\n",
    "\n",
    "for name, model in zip(names, models):\n",
    "    means_train = dists_train[name].mean\n",
    "    means_val = dists_val[name].mean\n",
    "    means_test = dists_test[name].mean\n",
    "    rmse_train[name] = rmse(y_train, means_train)\n",
    "    rmse_val[name] = rmse(y_val, means_val)\n",
    "    rmse_test[name] = rmse(y_test, means_test)\n",
    "\n",
    "for rmse_dict, df_name in zip(\n",
    "    [rmse_train, rmse_val, rmse_test], [\"training\", \"validation\", \"test\"]\n",
    "):\n",
    "    print(f\"RMSE on {df_name} set\")\n",
    "    for name, model in zip(names, models):\n",
    "        print(f\"{name}: {rmse_dict[name].mean():.4f}\")\n",
    "    print(f\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_wilcoxon_test(dists_val, Y_val, \"Validation\")\n",
    "rmse_wilcoxon_test(dists_test, Y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 90 Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_90_train = {}\n",
    "ql_90_val = {}\n",
    "ql_90_test = {}\n",
    "\n",
    "for features, response, dataset_name, ql_dict in zip(\n",
    "    [X_train, X_val, X_test],\n",
    "    [y_train, y_val, y_test],\n",
    "    [\"Training\", \"Validation\", \"Test\"],\n",
    "    [ql_90_train, ql_90_val, ql_90_test],\n",
    "):\n",
    "    print(f\"{dataset_name} Dataset Quantile Loss(es)\")\n",
    "    for model, model_name in zip(models, names):\n",
    "        ql_dict[model_name] = (\n",
    "            quantile_losses_raw(  ## TODO from PL: ED to check - this originally didn't have \"raw\"\n",
    "                0.9,\n",
    "                model,\n",
    "                model_name,\n",
    "                features,\n",
    "                response,\n",
    "                max_iter=1000,\n",
    "                tolerance=1e-4,\n",
    "                l=torch.Tensor([0]),\n",
    "                u=torch.Tensor(\n",
    "                    [np.max(y_train) + 3 * (np.max(y_train) - np.min(y_train))]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    print(f\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = (glm, cann, mdn, ddr, drn)\n",
    "ql90_wilcoxon_test(models, X_val, Y_val, y_train, \"Validation\")\n",
    "ql90_wilcoxon_test(models, X_test, Y_test, y_train, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(\n",
    "    nlls_val,\n",
    "    crps_val,\n",
    "    rmse_val,\n",
    "    ql_90_val,\n",
    "    nlls_test,\n",
    "    crps_test,\n",
    "    rmse_test,\n",
    "    ql_90_test,\n",
    "    model_names,\n",
    "    label_txt=\"Evaluation Metrics Table\",\n",
    "    caption_txt=\"Evaluation Metrics Table.\",\n",
    "    scaling_factor=1.0,\n",
    "):\n",
    "    header_row = (\n",
    "        \"\\\\begin{center}\\n\"\n",
    "        + \"\\captionof{table}{\"\n",
    "        + f\"{caption_txt}\"\n",
    "        + \"}\\n\"\n",
    "        + \"\\label{\"\n",
    "        + f\"{label_txt}\"\n",
    "        + \"}\\n\"\n",
    "        + \"\\scalebox{\"\n",
    "        + f\"{scaling_factor}\"\n",
    "        + \"}{\\n\"\n",
    "        + \"\\\\begin{tabular}{l|cccc|cccc}\\n\\\\toprule\\n\\\\toprule\\n\"\n",
    "        + \"&  \\multicolumn{4}{c}{$\\mathcal{D}_{\\\\text{Validation}}$}\"\n",
    "        + \"& \\multicolumn{4}{c}{ $\\mathcal{D}_{\\\\text{Test}}$}\\\\\\\\ \\n\"\n",
    "        + \" \\cmidrule{2-5}  \\cmidrule{6-9} $\\\\text{Model}$ $\\\\backslash$ $\\\\text{Metrics}$\"\n",
    "        + \" & NLL & CRPS & RMSE & 90\\% QL & NLL & CRPS & RMSE & 90\\% QL \\\\\\\\ \\\\midrule\"\n",
    "    )\n",
    "    rows = [header_row]\n",
    "\n",
    "    for name in model_names:\n",
    "        row = (\n",
    "            f\"{name} &  {(nlls_val[name].mean()):.4f}\"\n",
    "            f\" &  {(crps_val[name].mean()):.4f} \"\n",
    "            f\" & {(rmse_val[name].mean()):.4f} \"\n",
    "            f\" & {(ql_90_val[name].mean()):.4f} \"\n",
    "            f\" & {(nlls_test[name].mean()):.4f} \"\n",
    "            f\" & {(crps_test[name].mean()):.4f} \"\n",
    "            f\" & {(rmse_test[name].mean()):.4f} \"\n",
    "            f\" & {(ql_90_test[name].mean()):.4f} \\\\\\\\ \"\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    table = (\n",
    "        \"\\n\".join(rows)\n",
    "        + \"\\n\\\\bottomrule\\n\\\\bottomrule\"\n",
    "        + \"\\n\\\\end{tabular}\"\n",
    "        + \"\\n}\"\n",
    "        + \"\\n\\end{center}\"\n",
    "    )\n",
    "    return table\n",
    "\n",
    "\n",
    "latex_table = generate_latex_table(\n",
    "    nlls_val,\n",
    "    crps_val,\n",
    "    rmse_val,\n",
    "    ql_90_val,\n",
    "    nlls_test,\n",
    "    crps_test,\n",
    "    rmse_test,\n",
    "    ql_90_test,\n",
    "    names,\n",
    "    label_txt=\"Evaluation Metrics\",\n",
    "    caption_txt=\"Model comparisons based on various evaluation metrics.\",\n",
    "    scaling_factor=0.95,\n",
    ")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_residuals_plots(quantile_points(cdfs_test, y_test, grid))\n",
    "plt.savefig(PLOT_DIR / \"Quantile Residuals Plot Real.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_plot(cdfs_test, y_test, grid)\n",
    "plt.savefig(PLOT_DIR / \"Calibration Plot Real.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sec 6.4: Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 6.4.1 Local Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Extreme Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_diff = drn.distributions(X_test).mean - glm.distributions(X_test).mean\n",
    "# Find the top 5 values and their indices\n",
    "values, indices = torch.topk(means_diff.view(-1), 3, sorted=True)\n",
    "multi_indices = np.unravel_index(indices.numpy(), means_diff.shape)\n",
    "\n",
    "print(multi_indices, means_diff[multi_indices])\n",
    "idx_first = multi_indices[0][0]\n",
    "idx_second = multi_indices[0][1]\n",
    "idx_third = multi_indices[0][2]\n",
    "y_test.values[multi_indices], drn.distributions(X_test).mean[\n",
    "    multi_indices\n",
    "], glm.distributions(X_test).mean[multi_indices],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drn_explainer = DRNExplainer(\n",
    "    drn,\n",
    "    glm,\n",
    "    drn.cutpoints,\n",
    "    x_train_raw,\n",
    "    cat_features,\n",
    "    all_categories,\n",
    "    column_transformer=ct,\n",
    ")\n",
    "idx = idx_second\n",
    "drn_explainer.plot_dp_adjustment_shap(\n",
    "    instance_raw=x_test_raw.iloc[idx : (idx + 1)],\n",
    "    method=\"Kernel\",\n",
    "    nsamples_background_fraction=0.5,\n",
    "    top_K_features=5,\n",
    "    labelling_gap=0.1,\n",
    "    dist_property=\"Mean\",\n",
    "    x_range=(0.0, 50.0),\n",
    "    y_range=(0.0, 0.75),\n",
    "    observation=Y_test[idx : (idx + 1)],\n",
    "    density_transparency=0.5,\n",
    "    adjustment=True,\n",
    "    shap_fontsize=15,\n",
    "    figsize=(7, 7),\n",
    "    plot_title=\"Explaining a Large Mean Adjustment\",\n",
    "    legend_loc=\"upper left\",\n",
    ")\n",
    "\n",
    "plt.savefig(PLOT_DIR / \"(Real) Mean Adjustment SHAP.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drn_explainer = DRNExplainer(\n",
    "    drn,\n",
    "    glm,\n",
    "    drn.cutpoints,\n",
    "    x_train_raw,\n",
    "    cat_features,\n",
    "    all_categories,\n",
    "    column_transformer=ct,\n",
    ")\n",
    "idx = idx_second\n",
    "drn_explainer.plot_dp_adjustment_shap(\n",
    "    instance_raw=x_test_raw.iloc[idx : (idx + 1)],\n",
    "    method=\"Kernel\",\n",
    "    nsamples_background_fraction=0.5,\n",
    "    top_K_features=5,\n",
    "    labelling_gap=0.1,\n",
    "    dist_property=\"Mean\",\n",
    "    # other_df_models = [mdn, ddr], model_names = [\"MDN\", \"DDR\"],\\\n",
    "    x_range=(0.0, 50.0),\n",
    "    y_range=(0.0, 0.75),\n",
    "    observation=Y_test[idx : (idx + 1)],\n",
    "    density_transparency=0.5,\n",
    "    adjustment=False,\n",
    "    shap_fontsize=15,\n",
    "    figsize=(7, 7),\n",
    "    plot_title=\"Explaining a Large Mean Prediction\",\n",
    "    legend_loc=\"upper left\",\n",
    ")\n",
    "\n",
    "plt.savefig(PLOT_DIR / \"(Real) Mean Explanation SHAP.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Average Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean differences between DRN and GLM predictions\n",
    "means_diff = drn.distributions(X_test).mean - glm.distributions(X_test).mean\n",
    "\n",
    "# Find indices where the percentage change between means is betweenn than 30% and 50%\n",
    "valid_indices = (0.4 < (torch.abs(means_diff) / glm.distributions(X_test).mean)) & (\n",
    "    0.8 > (torch.abs(means_diff) / glm.distributions(X_test).mean)\n",
    ")\n",
    "\n",
    "# Filter X_test, x_test_raw, and y_test based on these valid indices\n",
    "X_test_new = X_test[valid_indices]\n",
    "x_test_raw_new = x_test_raw.iloc[valid_indices.numpy()]\n",
    "y_test_new = Y_test[valid_indices]\n",
    "\n",
    "# Recalculate mean differences with the filtered dataset\n",
    "means_diff_new = drn.distributions(X_test_new).mean - glm.distributions(X_test_new).mean\n",
    "glm_means_new = glm.distributions(X_test_new).mean\n",
    "\n",
    "# Find instances where the GLM predictions are close to the actual y values\n",
    "y_diff = torch.abs(y_test_new - glm_means_new)\n",
    "close_y_indices = y_diff < 0.3 * torch.abs(\n",
    "    y_test_new\n",
    ")  # Assuming 30% closeness threshold\n",
    "\n",
    "# Filter further based on the closeness of GLM predictions to y_test\n",
    "X_test_final = X_test_new[close_y_indices]\n",
    "means_diff_final = means_diff_new[close_y_indices]\n",
    "y_test_final = y_test_new[close_y_indices]\n",
    "glm_means_final = glm_means_new[close_y_indices]\n",
    "drn_means_final = drn.distributions(X_test_final).mean\n",
    "\n",
    "# Ensure we have enough data points after filtering\n",
    "if len(means_diff_final) >= 4:\n",
    "    # Find the top 4 values and their indices based on the filtered dataset\n",
    "    values, indices = torch.topk(torch.abs(means_diff_final).view(-1), 4, largest=False)\n",
    "\n",
    "    # Extract the original indices for the closest Four instances\n",
    "    original_indices = valid_indices.nonzero(as_tuple=True)[0][close_y_indices][indices]\n",
    "\n",
    "    idx_first = original_indices[0].item()\n",
    "    idx_second = original_indices[1].item()\n",
    "    idx_third = original_indices[2].item()\n",
    "    idx_forth = original_indices[3].item()\n",
    "\n",
    "    # Output the results for the selected instances\n",
    "    print(\"Original Indices:\", idx_first, idx_second)\n",
    "    print(\"Actual y values:\", y_test.values[idx_first], y_test.values[idx_second])\n",
    "    print(\n",
    "        \"DRN mean predictions:\",\n",
    "        drn.distributions(X_test).mean[idx_first].item(),\n",
    "        drn.distributions(X_test).mean[idx_second].item(),\n",
    "    )\n",
    "    print(\n",
    "        \"GLM mean predictions:\",\n",
    "        glm.distributions(X_test).mean[idx_first].item(),\n",
    "        glm.distributions(X_test).mean[idx_second].item(),\n",
    "    )\n",
    "else:\n",
    "    print(\"Not enough data points meet the criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drn_explainer = DRNExplainer(\n",
    "    drn,\n",
    "    glm,\n",
    "    drn.cutpoints,\n",
    "    x_train_raw,\n",
    "    cat_features,\n",
    "    all_categories,\n",
    "    column_transformer=ct,\n",
    ")\n",
    "idx = idx_first\n",
    "drn_explainer.plot_dp_adjustment_shap(\n",
    "    instance_raw=x_test_raw.iloc[idx : (idx + 1)],\n",
    "    method=\"Kernel\",\n",
    "    nsamples_background_fraction=0.5,\n",
    "    top_K_features=5,\n",
    "    labelling_gap=0.1,\n",
    "    dist_property=\"Mean\",\n",
    "    # other_df_models = [mdn, ddr], model_names = [\"MDN\", \"DDR\"],\\\n",
    "    x_range=(0.0, 6.5),\n",
    "    y_range=(0.0, 2.5),\n",
    "    observation=Y_test[idx : (idx + 1)],\n",
    "    density_transparency=0.5,\n",
    "    adjustment=True,\n",
    "    shap_fontsize=15,\n",
    "    figsize=(7, 7),\n",
    "    plot_title=\"Explaining an Average Mean Adjustment\",\n",
    "    legend_loc=\"upper left\",\n",
    ")\n",
    "\n",
    "plt.savefig(PLOT_DIR / \"(Real) Average Mean Adjustment SHAP.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Mild Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_diff = drn.distributions(X_test).mean - glm.distributions(X_test).mean\n",
    "\n",
    "valid_indices = (0.1 < (torch.abs(means_diff) / glm.distributions(X_test).mean)) & (\n",
    "    0.3 > (torch.abs(means_diff) / glm.distributions(X_test).mean)\n",
    ")\n",
    "\n",
    "X_test_new = X_test[valid_indices]\n",
    "x_test_raw_new = x_test_raw.iloc[valid_indices.numpy()]\n",
    "y_test_new = Y_test[valid_indices]\n",
    "\n",
    "means_diff_new = drn.distributions(X_test_new).mean - glm.distributions(X_test_new).mean\n",
    "glm_means_new = glm.distributions(X_test_new).mean\n",
    "\n",
    "y_diff = torch.abs(y_test_new - glm_means_new)\n",
    "close_y_indices = y_diff < 0.2 * torch.abs(\n",
    "    y_test_new\n",
    ")  # Assuming 20% closeness threshold\n",
    "\n",
    "X_test_final = X_test_new[close_y_indices]\n",
    "means_diff_final = means_diff_new[close_y_indices]\n",
    "y_test_final = y_test_new[close_y_indices]\n",
    "glm_means_final = glm_means_new[close_y_indices]\n",
    "drn_means_final = drn.distributions(X_test_final).mean\n",
    "\n",
    "if len(means_diff_final) >= 4:\n",
    "    values, indices = torch.topk(torch.abs(means_diff_final).view(-1), 4, largest=False)\n",
    "    original_indices = valid_indices.nonzero(as_tuple=True)[0][close_y_indices][indices]\n",
    "\n",
    "    idx_first = original_indices[0].item()\n",
    "    idx_second = original_indices[1].item()\n",
    "    idx_third = original_indices[2].item()\n",
    "    idx_forth = original_indices[3].item()\n",
    "\n",
    "    # Output the results for the selected instances\n",
    "    print(\"Original Indices:\", idx_first, idx_second)\n",
    "    print(\"Actual y values:\", y_test.values[idx_first], y_test.values[idx_second])\n",
    "    print(\n",
    "        \"DRN mean predictions:\",\n",
    "        drn.distributions(X_test).mean[idx_first].item(),\n",
    "        drn.distributions(X_test).mean[idx_second].item(),\n",
    "    )\n",
    "    print(\n",
    "        \"GLM mean predictions:\",\n",
    "        glm.distributions(X_test).mean[idx_first].item(),\n",
    "        glm.distributions(X_test).mean[idx_second].item(),\n",
    "    )\n",
    "else:\n",
    "    print(\"Not enough data points meet the criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drn_explainer = DRNExplainer(\n",
    "    drn,\n",
    "    glm,\n",
    "    drn.cutpoints,\n",
    "    x_train_raw,\n",
    "    cat_features,\n",
    "    all_categories,\n",
    "    column_transformer=ct,\n",
    ")\n",
    "idx = idx_second\n",
    "drn_explainer.plot_dp_adjustment_shap(\n",
    "    instance_raw=x_test_raw.iloc[idx : (idx + 1)],\n",
    "    method=\"Kernel\",\n",
    "    nsamples_background_fraction=0.5,\n",
    "    top_K_features=5,\n",
    "    labelling_gap=0.1,\n",
    "    dist_property=\"Mean\",\n",
    "    # other_df_models = [mdn, ddr], model_names = [\"MDN\", \"DDR\"],\\\n",
    "    x_range=(0.0, 6.0),\n",
    "    y_range=(0.0, 2.0),\n",
    "    observation=Y_test[idx : (idx + 1)],\n",
    "    density_transparency=0.5,\n",
    "    adjustment=True,\n",
    "    shap_fontsize=15,\n",
    "    figsize=(7, 7),\n",
    "    plot_title=\"Explaining a Mild Mean Adjustment\",\n",
    "    legend_loc=\"upper left\",\n",
    ")\n",
    "\n",
    "plt.savefig(PLOT_DIR / \"(Real) Mild Mean Adjustment SHAP.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CDF Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drn_explainer = DRNExplainer(\n",
    "    drn,\n",
    "    glm,\n",
    "    drn.cutpoints,\n",
    "    x_train_raw,\n",
    "    cat_features,\n",
    "    all_categories,\n",
    "    column_transformer=ct,\n",
    ")\n",
    "\n",
    "idx = idx_first\n",
    "drn_explainer.cdf_plot(\n",
    "    instance=x_test_raw.iloc[idx : (idx + 1)],\n",
    "    nsamples_background_fraction=0.2,\n",
    "    top_K_features=5,\n",
    "    labelling_gap=0.1,\n",
    "    dist_property=\"90% Quantile\",\n",
    "    quantile_bounds=(\n",
    "        torch.Tensor([drn.cutpoints[0]]),\n",
    "        torch.Tensor([drn.cutpoints[-1]]),\n",
    "    ),\n",
    "    x_range=(0.0, 8.5),\n",
    "    y_range=(0.0, 1.0),\n",
    "    density_transparency=0.9,\n",
    "    adjustment=True,\n",
    "    shap_fontsize=15,\n",
    "    figsize=(7, 7),\n",
    "    plot_title=\"90% Quantile Adjustment Explanation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sec 6.4.2: Global Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drn_explainer = DRNExplainer(\n",
    "    drn, glm, drn.cutpoints, x_train_raw, cat_features, all_categories, ct\n",
    ")\n",
    "kernel_shap_drn = drn_explainer.kernel_shap(\n",
    "    explaining_data=x_test_raw,\n",
    "    distributional_property=\"Mean\",\n",
    "    nsamples_background_fraction=0.2,\n",
    "    adjustment=True,\n",
    "    glm_output=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.global_importance_plot(num_features + cat_features, output=\"drn\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"(Real) Global Importance.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.beeswarm_plot(num_features + cat_features, output=\"drn\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"(Real) Beeswarm Summary Plot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.shap_dependence_plot((\"MariStat\", \"Exposure\"), output=\"drn\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"(SHAP Dependence) MariState X Exposure.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.shap_dependence_plot((\"MariStat\", \"LicAge\"), output=\"drn\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"(SHAP Dependence) MariState X LicAge.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.global_importance_plot(num_features + cat_features, output=\"value\")\n",
    "plt.savefig(PLOT_DIR / \"(Real Adjustment) Global Importance.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.beeswarm_plot(num_features + cat_features, output=\"value\")\n",
    "plt.savefig(PLOT_DIR / \"(Real Adjustment) Beeswarm Summary Plot.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.shap_dependence_plot((\"VehEnergy\", \"Exposure\"), output=\"value\")\n",
    "plt.savefig(PLOT_DIR / \"(SHAP Adjustment) VehEnergy X Exposure.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.shap_dependence_plot((\"RiskVar\", \"Exposure\"), output=\"value\")\n",
    "plt.savefig(PLOT_DIR / \"(SHAP Adjustment) RiskVar X Exposure.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_shap_drn.shap_dependence_plot((\"LicAge\", \"DrivAge\"), output=\"value\")\n",
    "plt.savefig(PLOT_DIR / \"(SHAP Adjustment) LicAge X DrivAge.png\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
