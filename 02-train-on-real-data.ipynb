{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skopt\n",
    "import torch\n",
    "from drn import *\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "\n",
    "from hyperparameter_tuning_objectives import (\n",
    "    objective_cann,\n",
    "    objective_ddr,\n",
    "    objective_drn,\n",
    "    objective_mdn,\n",
    ")\n",
    "\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path(\"models/real\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_opts = {\n",
    "    \"n_calls\": 200,\n",
    "    \"n_random_starts\": 25,\n",
    "    \"random_state\": 0,\n",
    "    \"verbose\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = \"data/freMPL1.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "claims = df.loc[df[\"ClaimAmount\"] > 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "target = claims[\"ClaimAmount\"] / 1000\n",
    "features = claims.drop(\"ClaimAmount\", axis=1)\n",
    "features = features.drop(\n",
    "    [\"RecordBeg\", \"RecordEnd\", \"ClaimInd\", \"Garage\"], axis=1\n",
    ")  # Drop garage due to missing values\n",
    "\n",
    "# Convert \"VehAge\" categories to numeric\n",
    "features[\"VehAge\"] = features[\"VehAge\"].map(\n",
    "    {\n",
    "        \"0\": 0,\n",
    "        \"1\": 1,\n",
    "        \"2\": 2,\n",
    "        \"3\": 3,\n",
    "        \"4\": 4,\n",
    "        \"5\": 5,\n",
    "        \"6-7\": 6,\n",
    "        \"8-9\": 8,\n",
    "        \"10+\": 11,\n",
    "    }\n",
    ")\n",
    "feature_names = features.columns\n",
    "\n",
    "speed_ranges = [speed for speed in np.unique(features[\"VehMaxSpeed\"])]\n",
    "speed_series = pd.Series(speed_ranges)\n",
    "mapping = {speed_range: i + 1 for i, speed_range in enumerate(speed_ranges)}\n",
    "features[\"VehMaxSpeed\"] = features[\"VehMaxSpeed\"].map(mapping)\n",
    "features[\"SocioCateg\"] = features[\"SocioCateg\"].str.extract(\"(\\d+)\").astype(int)\n",
    "\n",
    "cat_features = [\n",
    "    \"HasKmLimit\",\n",
    "    \"Gender\",\n",
    "    \"MariStat\",\n",
    "    \"VehUsage\",\n",
    "    \"VehBody\",\n",
    "    \"VehPrice\",\n",
    "    \"VehEngine\",\n",
    "    \"VehEnergy\",\n",
    "    \"VehClass\",\n",
    "    \"SocioCateg\",\n",
    "]\n",
    "\n",
    "num_features = [feature for feature in features.columns if feature not in cat_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and preprocess the data\n",
    "(\n",
    "    x_train,\n",
    "    x_val,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    x_train_raw,\n",
    "    x_val_raw,\n",
    "    x_test_raw,\n",
    "    num_features,\n",
    "    cat_features,\n",
    "    all_categories,\n",
    "    ct,\n",
    ") = split_and_preprocess(features, target, num_features, cat_features, seed=0)\n",
    "\n",
    "# Calculate and print statistics for y_train, y_val, y_test\n",
    "np.max(y_train), np.median(y_train), np.max(y_val), np.median(y_val), np.max(\n",
    "    y_test\n",
    "), np.median(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(x_train.values)\n",
    "Y_train = torch.Tensor(y_train.values)\n",
    "X_val = torch.Tensor(x_val.values)\n",
    "Y_val = torch.Tensor(y_val.values)\n",
    "X_test = torch.Tensor(x_test.values)\n",
    "Y_test = torch.Tensor(y_test.values)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = \"gamma\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "glm = GLM(X_train.shape[1], distribution=distribution)\n",
    "train(\n",
    "    glm,\n",
    "    gaussian_deviance_loss if distribution == \"gaussian\" else gamma_deviance_loss,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    log_interval=10,\n",
    "    epochs=5000,\n",
    "    lr=0.001,\n",
    "    patience=100,\n",
    "    batch_size=100,\n",
    ")\n",
    "glm.update_dispersion(X_train, Y_train)\n",
    "torch.save(glm, MODEL_DIR / \"glm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_cann_real = [\n",
    "    Integer(1, 6, name=\"num_hidden_layers\"),\n",
    "    Categorical([32, 64, 128, 256, 512], name=\"hidden_size\"),\n",
    "    Real(0.0, 0.5, name=\"dropout_rate\"),\n",
    "    Real(0.0001, 0.01, name=\"lr\"),\n",
    "    Categorical([64, 128, 256, 512], name=\"batch_size\"),\n",
    "]\n",
    "\n",
    "# Run Bayesian optimization\n",
    "res_cann_real = skopt.gp_minimize(\n",
    "    lambda params: objective_cann(\n",
    "        params, X_train, Y_train, X_val, Y_val, train_dataset, val_dataset, glm, \"gamma\"\n",
    "    ),\n",
    "    space_cann_real,\n",
    "    **hp_opts,\n",
    ")\n",
    "with open(MODEL_DIR / \"cann_hp.pkl\", \"wb\") as f:\n",
    "    res_cann_real.specs['args'].pop('func')\n",
    "    pickle.dump(res_cann_real, f)\n",
    "best_cann_params = res_cann_real.x\n",
    "print(best_cann_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "cann = CANN(\n",
    "    glm,\n",
    "    num_hidden_layers=int(best_cann_params[0]),\n",
    "    hidden_size=int(best_cann_params[1]),\n",
    "    dropout_rate=best_cann_params[2],\n",
    ")\n",
    "train(\n",
    "    cann,\n",
    "    gaussian_deviance_loss if distribution == \"gaussian\" else gamma_deviance_loss,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs=2000,\n",
    "    lr=best_cann_params[-2],\n",
    "    patience=50,\n",
    "    batch_size=int(best_cann_params[-1]),\n",
    ")\n",
    "cann.update_dispersion(X_train, Y_train)\n",
    "torch.save(cann, MODEL_DIR / \"cann.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_mdn_real = [\n",
    "    Integer(1, 6, name=\"num_hidden_layers\"),\n",
    "    Categorical([32, 64, 128, 256, 512], name=\"hidden_size\"),\n",
    "    Real(0.0, 0.5, name=\"dropout_rate\"),\n",
    "    Real(0.0001, 0.01, name=\"lr\"),\n",
    "    Integer(2, 10, name=\"num_components\"),\n",
    "    Categorical([64, 128, 256, 512], name=\"batch_size\"),\n",
    "]\n",
    "\n",
    "# Run Bayesian optimization\n",
    "res_mdn_real = skopt.gp_minimize(\n",
    "    lambda params: objective_mdn(\n",
    "        params, X_train, Y_train, X_val, Y_val, train_dataset, val_dataset, \"gamma\"\n",
    "    ),\n",
    "    space_mdn_real,\n",
    "    **hp_opts,\n",
    ")\n",
    "with open(MODEL_DIR / \"mdn_hp.pkl\", \"wb\") as f:\n",
    "    res_mdn_real.specs['args'].pop('func')\n",
    "    pickle.dump(res_mdn_real, f)\n",
    "best_mdn_params = res_mdn_real.x\n",
    "print(best_mdn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "mdn = MDN(\n",
    "    X_train.shape[1],\n",
    "    num_components=int(best_mdn_params[-2]),\n",
    "    hidden_size=int(best_mdn_params[1]),\n",
    "    num_hidden_layers=int(best_mdn_params[0]),\n",
    "    dropout_rate=best_mdn_params[2],\n",
    "    distribution=distribution,\n",
    ")\n",
    "train(\n",
    "    mdn,\n",
    "    gaussian_mdn_loss if distribution == \"gaussian\" else gamma_mdn_loss,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    lr=best_mdn_params[3],\n",
    "    batch_size=int(best_mdn_params[-1]),\n",
    "    epochs=2000,\n",
    "    patience=50,\n",
    ")\n",
    "torch.save(mdn, MODEL_DIR / \"mdn.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_ddr_real = [\n",
    "    Integer(1, 6, name=\"num_hidden_layers\"),\n",
    "    Categorical([32, 64, 128, 256, 512], name=\"hidden_size\"),\n",
    "    Real(0.0, 0.5, name=\"dropout_rate\"),\n",
    "    Real(0.0002, 0.01, name=\"lr\"),\n",
    "    Categorical([0.05, 0.075, 0.1, 0.125, 0.15], name=\"proportion\"),\n",
    "    Categorical([64, 128, 256, 512], name=\"batch_size\"),\n",
    "]\n",
    "\n",
    "# Run Bayesian optimization\n",
    "res_ddr_real = skopt.gp_minimize(\n",
    "    lambda params: objective_ddr(\n",
    "        params, X_train, Y_train, X_val, Y_val, train_dataset, val_dataset\n",
    "    ),\n",
    "    space_ddr_real,\n",
    "    **hp_opts,\n",
    ")\n",
    "with open(MODEL_DIR / \"ddr_hp.pkl\", \"wb\") as f:\n",
    "    res_ddr_real.specs['args'].pop('func')\n",
    "    pickle.dump(res_ddr_real, f)\n",
    "best_ddr_params = res_ddr_real.x\n",
    "print(best_ddr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "cutpoints_DDR = ddr_cutpoints(\n",
    "    c_0=(y_train.min() * 1.05 if y_train.min() < 0 else 0.0),\n",
    "    c_K=y_train.max() * 1.05,\n",
    "    y=y_train,\n",
    "    p=best_ddr_params[-2],\n",
    ")\n",
    "ddr = DDR(\n",
    "    X_train.shape[1],\n",
    "    cutpoints=cutpoints_DDR,\n",
    "    hidden_size=int(best_ddr_params[1]),\n",
    "    num_hidden_layers=int(best_ddr_params[0]),\n",
    "    dropout_rate=best_ddr_params[2],\n",
    ")\n",
    "torch.manual_seed(23)\n",
    "train(\n",
    "    ddr,\n",
    "    ddr_loss,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    lr=0.0005,\n",
    "    batch_size=100,\n",
    "    log_interval=1,\n",
    "    patience=30,\n",
    "    epochs=1000,\n",
    ")\n",
    "torch.save(ddr, MODEL_DIR / \"ddr.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_drn_real = [\n",
    "    Integer(1, 6, name=\"num_hidden_layers\"),\n",
    "    Categorical([32, 64, 128, 256, 512], name=\"hidden_size\"),\n",
    "    Real(0.0, 0.5, name=\"dropout_rate\"),\n",
    "    Real(0.0002, 0.01, name=\"lr\", prior=\"log-uniform\"),\n",
    "    Real(1e-6, 1e-1, name=\"kl_alpha\", prior=\"log-uniform\"),\n",
    "    Categorical([0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], name=\"mean_alpha\"),\n",
    "    Categorical([0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], name=\"dv_alpha\"),\n",
    "    Categorical([64, 128, 256, 512], name=\"batch_size\"),\n",
    "    Categorical([0.1, 0.125, 0.15], name=\"proportion\"),\n",
    "    Categorical([1, 3, 5], name=\"min_observation\"),\n",
    "]\n",
    "\n",
    "# Run Bayesian optimization\n",
    "res_drn_real = skopt.gp_minimize(\n",
    "    lambda params: objective_drn(\n",
    "        params,\n",
    "        X_train=X_train,\n",
    "        Y_train=Y_train,\n",
    "        X_val=X_val,\n",
    "        Y_val=Y_val,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        glm=glm,\n",
    "        patience=50,\n",
    "        kl_direction=\"forwards\",\n",
    "    ),\n",
    "    space_drn_real,\n",
    "    **hp_opts,\n",
    ")\n",
    "with open(MODEL_DIR / \"drn_hp.pkl\", \"wb\") as f:\n",
    "    res_drn_real.specs['args'].pop('func')\n",
    "    pickle.dump(res_drn_real, f)\n",
    "best_drn_params = res_drn_real.x\n",
    "print(best_drn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)\n",
    "cutpoints_DRN = drn_cutpoints(\n",
    "    c_0=(y_train.min() * 1.05 if y_train.min() < 0 else 0.0),\n",
    "    c_K=y_train.max() * 1.05,\n",
    "    p=best_drn_params[-2],\n",
    "    y=y_train,\n",
    "    min_obs=int(best_drn_params[-1]),\n",
    ")\n",
    "drn = DRN(\n",
    "    num_features=X_train.shape[1],\n",
    "    cutpoints=cutpoints_DRN,\n",
    "    glm=glm,\n",
    "    hidden_size=int(best_drn_params[1]),\n",
    "    num_hidden_layers=int(best_drn_params[0]),\n",
    "    baseline_start=False,\n",
    "    dropout_rate=best_drn_params[2],\n",
    ")\n",
    "train(\n",
    "    model=drn,\n",
    "    criterion=lambda pred, y: drn_loss(\n",
    "        pred,\n",
    "        y,\n",
    "        kl_alpha=best_drn_params[4],  # 2e-4\n",
    "        mean_alpha=best_drn_params[5],\n",
    "        tv_alpha=0,\n",
    "        dv_alpha=best_drn_params[6],\n",
    "    ),\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    batch_size=int(best_drn_params[7]),\n",
    "    epochs=2000,\n",
    "    patience=50,\n",
    "    lr=best_drn_params[3],\n",
    "    print_details=True,\n",
    "    log_interval=1,\n",
    ")\n",
    "torch.save(drn, MODEL_DIR / \"drn.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
